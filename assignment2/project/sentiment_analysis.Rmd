---
title: "Sentiment analysis"
output: html_notebook
---

<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Setup -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->

<!-- Set colours according to Stellenbosch Branding manual-->
<!-- http://stbweb01.stb.sun.ac.za/centenarybrandtoolkit/SU_Brand_Manual_2019.pdf -->
```{r set colours}
dailymav_colour <- "#FF8F1C" # Daily Maveric - arts_orange
capetalk_colour <- "#9e2629" # Cape Talk - law_winered
sabc_colour <- "#84329B" # SABC News - theology_purple
news24_colour <- "#326295" # News24 - education_darkblue
ewn_colour <- "#509e2f" # EWN - agri_green

# other colours to use
med_bluegreen <- "#006163"
military_salmon <- "#e56a54"
science_red <- "#cb333b"
ems_lightblue <- "#2dccd3"
maroon <- "#60223b"
mustard <- "#F1B434"
darkgrey <- "#333333"
lightgrey <- "#8c979a"
black <- "#000000"
engineering_yellow <- "#eaaa00"
```

```{r custom themes}
custom_column_basic_theme <- function () { 
    theme_minimal(base_size=12, base_family="Avenir") +
        theme(
            panel.background  = element_blank(),
            panel.grid.minor = element_blank(),
            legend.position="none", # no legend
            plot.title = element_text(size = 10, hjust = 0),
            plot.caption = element_text(size = 8, hjust = 0)
        )
}
```

```{r import libraries}
library(tidyverse)
library(vader) # sentiment analysis
library(tidytext)
library(lubridate) # date conversion
library(ggplot2)
```

```{r extract relevant data from read in csv}
extract_relevant_data <- function(df) {
  return(df %>% select(status_id, screen_name, created_at, statuses_count, is_retweet, favorite_count, retweet_count, followers_count, friends_count, text))
}
```

```{r read data}
CapeTalk_tweets <- extract_relevant_data(read.csv("data/CapeTalk_tweets.csv"))
DailyMav_tweets <- extract_relevant_data(read.csv("data/DailyMav_tweets.csv"))
EWN_tweets <- extract_relevant_data(read.csv("data/EWN_tweets.csv"))
News24_tweets <- extract_relevant_data(read.csv("data/News24_tweets.csv"))
SABCNews_tweets <- extract_relevant_data(read.csv("data/SABCNews_tweets.csv"))
media_outlet_list <- list(CapeTalk_tweets, DailyMav_tweets, EWN_tweets, News24_tweets, SABCNews_tweets)
head(media_outlet_list[1])
```

<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Data Wrangling -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->

```{r get relevant covid tweets}
# filter for tweets related to covid
relevant_CapeTalk <- CapeTalk_tweets[grepl("covid", CapeTalk_tweets[["text"]]) | 
                  grepl("Covid", CapeTalk_tweets[["text"]]) |
                  grepl("corona", CapeTalk_tweets[["text"]]) |
                  grepl("Corona", CapeTalk_tweets[["text"]]) |
                  grepl("Pandemic", CapeTalk_tweets[["text"]]) |
                  grepl("pandemic", CapeTalk_tweets[["text"]]), ]
relevant_CapeTalk <- relevant_CapeTalk %>%
  mutate(created_at = ymd_hms(created_at))

relevant_CapeTalk
```
xx
```{r remove stopwords and clean data}
# define custom stopwords
my_stopwords <- c("covid", "covid19", "corona", "coronavirus")

# remove stopwords, links, special chatacters
tidy_tweets <- relevant_CapeTalk %>%
  unnest_tokens(word, text) %>%
  filter(!word %in% stop_words$word,
         !word %in% my_stopwords,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]")) %>%
  # group words back together (creating sentences) according to their twitter_id
  group_by(status_id) %>%
  summarize(text=str_c(word, collapse = " ")) %>%
  ungroup()
tidy_tweets
```


<!-- x -->
<!-- ```{r remove stopwords} -->
<!-- # define custom stopwords -->
<!-- my_stopwords <- c("covid", "covid19", "corona", "coronavirus") -->
<!-- # remove stopwords -->
<!-- tidy_tweets <- relevant_CapeTalk %>% -->
<!--   # unnest_tokens(word, text) %>% -->
<!--   filter(!text %in% stop_words$word, -->
<!--          !text %in% my_stopwords, -->
<!--          !text %in% str_remove_all(stop_words$word, "'"), -->
<!--          str_detect(text, "[a-z]")) -->
<!-- tidy_tweets -->
<!-- ``` -->
<!-- xxx -->
<!-- ```{r clean data} -->
<!-- require("tm") -->
<!-- # remove links - https://towardsdatascience.com/twitter-sentiment-analysis-and-visualization-using-r-22e1f70f6967 -->
<!-- relevant_CapeTalk$text <- gsub("http\\S+", "", relevant_CapeTalk$text) -->
<!-- relevant_CapeTalk$text <- gsub("\\s+", " ", relevant_CapeTalk$text) -->
<!-- relevant_CapeTalk$text <- gsub("[[:punct:]]", "", relevant_CapeTalk$text) -->
<!-- relevant_CapeTalk$text <- gsub("['`^~\"]", " ", relevant_CapeTalk$text) -->
<!-- relevant_CapeTalk$text <- iconv(relevant_CapeTalk$text, from = "UTF-8", to = "ASCII//TRANSLIT//IGNORE") -->
<!-- relevant_CapeTalk$text <- gsub("['`^~\"]", "", relevant_CapeTalk$text) -->
<!-- # remove stopwards -->
<!-- text<-removeWords(text, stop_words) -->

<!-- relevant_CapeTalk$text -->
<!-- ``` -->



```{r vader sentiment analysis on dataframe}
vader_df(tidy_tweets$text)
```
<!-- xxx -->
<!-- ```{r vader sentiment analysis on one tweet} -->
<!-- get_vader(CapeTalk_tweets$text[3]) -->
<!-- ``` -->


<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Learning stuff - not for publishing -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->

# Learning: TidyTextMining

## Chapter 1: Tidy text format
```{r one sentence sentiment}
text <- c("Because I could not stop for Death -",
          "He kindly stopped for me -",
          "The Carriage held but just Ourselves -",
          "and Immortality")
text_df <- tibble(line = 1:4, text = text)
text_df %>% unnest_tokens(word, text)
```

```{r jane austen dataframe sentiment}
# For my sentiment analysis analysis
# book = media company
# line number = tweet ID
# chapter = tweet number in dataframe
# text = tweet text -> word = tweet words

library(janeaustenr)
original_books <- austen_books() %>%
  group_by(book) %>%
  # get line number and chapter in book
  mutate(linenumber = row_number(),
         chapter = cumsum(str_detect(text, 
                                     regex("^chapter [\\divxlc]",
                                           ignore_case = TRUE)))) %>%
  ungroup()

# unnest tokens of book
tidy_books <- original_books %>%
  unnest_tokens(word, text)
tidy_books

# remove stopwords
data("stop_words")
tidy_books <- tidy_books %>%
  anti_join(stop_words)
tidy_books
# get most commonly used words
tidy_books %>%
  count(word, sort = TRUE) 

# plot word frequency
tidy_books %>%
  count(word, sort = TRUE) %>%
  filter(n > 600) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```
```{r str extract method}
# We use str_extract() here because the UTF-8 encoded texts from Project Gutenberg have some examples of words with underscores around them to indicate emphasis (like italics).
# mutate(word = str_extract(word, "[a-z']+")) %>%
```


## Chapter 2: Sentiment analysis

```{r}
get_sentiments("afinn")
```


