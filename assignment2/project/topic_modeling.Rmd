---
title: "R Notebook"
output: html_notebook
---

## adapted from TidyTextMining Ch 6


```{r}
#library(tidyverse)
library(tidytext)
library(lubridate)
library(dplyr)
library(stringr)
library(tidyr)
library(topicmodels)
#library(scales)
```

```{r}
SABCNews_tweets <- read.csv("data/SABCNews_tweets.csv")
DailyMav_tweets <- read.csv("data/DailyMav_tweets.csv")
```
```{r}
# filter for tweets related to covid
relevant_SABC <- SABCNews_tweets[grepl("covid", SABCNews_tweets[["text"]]) | 
                  grepl("Covid", SABCNews_tweets[["text"]]) |
                  grepl("corona", SABCNews_tweets[["text"]]) |
                  grepl("Corona", SABCNews_tweets[["text"]]) |
                  grepl("Pandemic", SABCNews_tweets[["text"]]) |
                  grepl("pandemic", SABCNews_tweets[["text"]]), ]
relevant_SABC <- relevant_SABC %>%
  mutate(created_at = ymd_hms(created_at))

relevant_DM <- DailyMav_tweets[grepl("covid", DailyMav_tweets[["text"]]) | 
                  grepl("Covid", DailyMav_tweets[["text"]]) |
                  grepl("corona", DailyMav_tweets[["text"]]) |
                  grepl("Corona", DailyMav_tweets[["text"]]) |
                  grepl("Pandemic", DailyMav_tweets[["text"]]) |
                  grepl("pandemic", DailyMav_tweets[["text"]]), ]
relevant_DM <- relevant_DM %>%
  mutate(created_at = ymd_hms(created_at))
```

```{r}
# combine all tweets into one dataframe
tweets <- bind_rows(relevant_SABC %>% 
                      mutate(user = "SABC"),
                    relevant_DM %>% 
                      mutate(user = "DailyMaverick"))
```

```{r}
# define custom stopwords
my_stopwords <- c("covid", "covid19", "corona", "coronavirus")
# remove stopwords
tidy_tweets <- tweets %>% 
  unnest_tokens(word, text, token = "tweets") %>%
  filter(!word %in% stop_words$word,
         !word %in% my_stopwords,
         !word %in% str_remove_all(stop_words$word, "'"),
         str_detect(word, "[a-z]"))
```

# Topic Modelling

```{r}
# word count by media organisation
word_counts <- tidy_tweets %>%
  select(user, word, created_at) %>%
  count(user, word, sort = TRUE) %>%
  ungroup()

word_counts
```
```{r}
# create document term matrix
# (treating all tweets from one organisation as one document)
orgs_dtm <- word_counts %>%
  cast_dtm(user, word, n)

orgs_dtm
```
```{r}
# define number of topics
k_topics = 4
# apply topic modelling
orgs_lda <- LDA(orgs_dtm, k = k_topics, control = list(seed = 1234))
orgs_lda
```
```{r}
orgs_topics <- tidy(orgs_lda, matrix = "beta")
orgs_topics
```
```{r}
# find top words per topic
top_terms <- orgs_topics %>%
  group_by(topic) %>%
  slice_max(beta, n = 5) %>% 
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

```{r}
# plot top terms per topic
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggplot(aes(beta, term, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  scale_y_reordered()
```

# topic modelling over time

```{r}
# past_month being the full date of the first day of that month - eg "2021-06-01" for June
model_topics_of_month <- function(past_month, nr_topics) {
  # word count by media organisation for specified month
  word_counts <- tidy_tweets %>%
    # create time bins (monthly)
    mutate(time_floor = floor_date(created_at, unit = "month")) %>%
    select(user, word, time_floor) %>%
    # filter to specified month
    filter(time_floor == as.Date(past_month)) %>%
    count(user, word, sort = TRUE) %>%
    ungroup()
    
  # create document term matrix
  # (treating all tweets from one organisation as one document)
  orgs_dtm <- word_counts %>%
    cast_dtm(user, word, n)

  # apply topic modelling
  orgs_lda <- LDA(orgs_dtm, k = nr_topics, control = list(seed = 1234))

  orgs_topics <- tidy(orgs_lda, matrix = "beta")

  # find top words per topic
  top_terms <- orgs_topics %>%
    group_by(topic) %>%
    slice_max(beta, n = 5) %>% 
    ungroup() %>%
    arrange(topic, -beta)

  # plot top terms per topic
  top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    labs(title = paste("Topics for month starting", past_month)) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered()
}
```

```{r}
model_topics_of_month("2021-06-01", 4)
```
```{r}
model_topics_of_month("2021-05-01", 2)
# note: previous months have fewer topics because SABC only has data for June, anything before is only Daily Maverick
```
```{r}
model_topics_of_month("2021-04-01", 2)
```

# topic modelling per organisation

```{r}
# org being the name of the media agency as found in the tidy_tweets dataframe
model_topics_of_org <- function(org, nr_topics) {
  # word count for specified media organisation
  word_counts <- tidy_tweets %>%
    select(user, word) %>%
    # filter to specified org
    filter(user == org) %>%
    count(user, word, sort = TRUE) %>%
    ungroup()
    
  # create document term matrix
  # (treating all tweets from one organisation as one document)
  orgs_dtm <- word_counts %>%
    cast_dtm(user, word, n)

  # apply topic modelling
  orgs_lda <- LDA(orgs_dtm, k = nr_topics, control = list(seed = 1234))

  orgs_topics <- tidy(orgs_lda, matrix = "beta")

  # find top words per topic
  top_terms <- orgs_topics %>%
    group_by(topic) %>%
    slice_max(beta, n = 5) %>% 
    ungroup() %>%
    arrange(topic, -beta)

  # plot top terms per topic
  top_terms %>%
    mutate(term = reorder_within(term, beta, topic)) %>%
    ggplot(aes(beta, term, fill = factor(topic))) +
    geom_col(show.legend = FALSE) +
    labs(title = paste("Topics for ", org)) +
    facet_wrap(~ topic, scales = "free") +
    scale_y_reordered()
}
```
```{r}
model_topics_of_org("DailyMaverick", 3)
```
```{r}
model_topics_of_org("SABC", 2)
```


