---
title: "R Notebook"
output: html_notebook
---

## adapted from TidyTextMining Ch 7

<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Setup -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->

```{r set colours}
# Microsoft Sharepoint main colours
# https://docs.microsoft.com/en-us/sharepoint/dev/design/themes-colors
ms_red <- "#A4262C"
ms_orange <- "#CA5010"
ms_gold <- "#8F7034"
ms_green <- "#407855"
ms_teal <- "#038387"
ms_blue <- "#0078d4"
ms_darkblue <- "#40587C"
ms_indigo <- "#4052AB"
ms_plum <- "#854085"
ms_purple <- "#8764B8"
ms_coolgrey <- "#737373"
ms_warmgrey <- "#867365"

ms_colors <- c(ms_darkblue, ms_red, ms_orange, ms_teal, ms_gold, ms_coolgrey, ms_purple, ms_warmgrey, ms_green, ms_blue, ms_indigo, ms_plum)
```

```{r custom themes}
custom_column_basic_theme <- function () { 
    theme_minimal(base_size=12, base_family="Avenir") +
        theme(
            panel.background  = element_blank(),
            panel.grid.minor = element_blank(),
            legend.position="none", # no legend
            plot.title = element_text(size = 12, hjust = 0),
            plot.subtitle = element_text(size = 10, hjust = 0),
            axis.title.y = element_text(size = 10)
        )
}

custom_line_basic_theme <- function () { 
    theme_minimal(base_size=12, base_family="Avenir") +
        theme(
            panel.background  = element_blank(),
            panel.grid.minor = element_blank(),
            plot.title = element_text(size = 12, hjust = 0),
            plot.subtitle = element_text(size = 10, hjust = 0),
            axis.title.y = element_text(size = 10)
        )
}
```

```{r import libraries}
library(tidyverse)
library(tidytext)
library(lubridate)
library(dplyr)
library(stringr)
library(scales)
library(gridExtra)
library(grid)
```

```{r extract relevant data from read in csv}
extract_relevant_data <- function(df) {
  return(df %>% select(status_id, screen_name, name, location, description, created_at, statuses_count, is_retweet, favorite_count, retweet_count, followers_count, friends_count, text))
}
```

```{r read data}
# SA media organisations
SABCNews_tweets <- extract_relevant_data(read.csv("data/sabcnews.csv"))
DailyMav_tweets <- extract_relevant_data(read.csv("data/dailymaverick.csv"))
EWN_tweets <- extract_relevant_data(read.csv("data/ewnupdates.csv"))
News24_tweets <- extract_relevant_data(read.csv("data/news24.csv"))

SA_media_outlet_list <- list(DailyMav_tweets, EWN_tweets, News24_tweets, SABCNews_tweets)

head(SA_media_outlet_list[1])

# SA government
govZA_tweets <- extract_relevant_data(read.csv("data/governmentza.csv"))

# Global
nyTimes_tweets <- extract_relevant_data(read.csv("data/nytimes.csv"))
economist_tweets <- extract_relevant_data(read.csv("data/theeconomist.csv"))
bbcworld_tweets <- extract_relevant_data(read.csv("data/bbcworld.csv"))
AP_tweets <- extract_relevant_data(read.csv("data/ap.csv"))

global_media_outlet_list <- list(nyTimes_tweets, economist_tweets, bbcworld_tweets, AP_tweets)

head(global_media_outlet_list[1])
```

<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Data Wrangling -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->

```{r extract tweets related to covid and combine covid tweets into a list}
# extract covid tweets function
get_covid_tweets <- function(df) {
relevant_df <-  df[grepl("covid", df[["text"]]) | 
                  grepl("Covid", df[["text"]]) |
                  grepl("corona", df[["text"]]) |
                  grepl("Corona", df[["text"]]) |
                  grepl("Pandemic", df[["text"]]) |
                  grepl("pandemic", df[["text"]]), ]
relevant_df <- relevant_df %>%
  mutate(created_at = as.Date(created_at))
return(relevant_df)
}

# South African: combine relevant (Covid-related) tweets for all media outlets into list
covid_tweets_SA <- list(get_covid_tweets(DailyMav_tweets), 
                     get_covid_tweets(EWN_tweets), 
                     get_covid_tweets(News24_tweets), 
                     get_covid_tweets(SABCNews_tweets))

# SA government relevant (Covid-related) tweets
relevant_govZA <- get_covid_tweets(govZA_tweets)

# Global: combine relevant (Covid-related) tweets for all media outlets into list
covid_tweets_global <- list(get_covid_tweets(nyTimes_tweets), 
                            get_covid_tweets(AP_tweets),
                            get_covid_tweets(bbcworld_tweets), 
                            get_covid_tweets(economist_tweets))
```

```{r}
# combine all tweets into one dataframe
# Covid-related tweets
tweets_SA <- bind_rows(covid_tweets_SA)
tweets_global <- bind_rows(covid_tweets_global)
# all tweets
all_tweets_SA <- bind_rows(SA_media_outlet_list)
all_tweets_global <- bind_rows(global_media_outlet_list)
```

```{r tidy tweets - remove stopwords and clean data}
clean_tweets <- function(tweets_in) {
  # define custom stopwords
  my_stopwords <- c("covid", "covid19", "corona", "coronavirus")
  # remove stopwords
  tidy_tweets <- tweets_in %>% 
    unnest_tokens(word, text, token = "tweets") %>%
    filter(!word %in% stop_words$word, # remove normal english stopwords
           # remove normal english stopwords that would contain ' that was lost by unnesting
           # eg "ain't" is a stopwords but wouldn't be recognised as such because it was converted to "aint"
           !word %in% str_remove_all(stop_words$word, "'"), # remove remaining normal stopwords
           !word %in% my_stopwords, # remove custom stopwords
           !str_starts(word, "http"), # remove links
           !str_starts(word, "@"), # remove mentions
           str_detect(word, "^[a-z#]*$")) %>% # remove special characters
    mutate(word = gsub("\\d+", "", word)) # remove numbers

  return(tidy_tweets)
}

tidy_tweets_SA <- clean_tweets(tweets_SA)
tidy_tweets_global <- clean_tweets(tweets_global)
```

# Top words per organisation
```{r}
# top n words to consider
n_words <- 10
```

```{r frequency of each word}
# calculate the frequency by org of each word occuring in given dataframe
word_frequencies <- function(tidy_df) {
  frequencies <- tidy_df %>% 
  group_by(name) %>% # group by organisation
  count(word, sort = TRUE) %>% # count occurrences of each word
  left_join(tidy_df %>% 
              group_by(name) %>% # add the word count to the original df
              summarise(total = n())) %>% # add the total number of words used per org
  mutate(freq = n/total) %>% # add frequency of each word occurence (number of times the word occurs divided by the total number of words)
  ungroup()
}

word_frequency_SA <- word_frequencies(tidy_tweets_SA)
word_frequency_global <- word_frequencies(tidy_tweets_global)

word_frequency_SA
word_frequency_global
```


```{r top n words per organisation}
top_words <- function(word_frequency_df, n_top_words) {
  # top n words per organisation
  top_n <- word_frequency_df %>%
    group_by(name) %>% # group df by organisation
    slice_max(order_by = freq, n=n_top_words) %>% # extract n_top_words most frequent words used per org
    ungroup
  
  users <- unique(top_n[c("name")]) # names of the orgs
  
  # Finding maximum length
  #ln <- top_n %>%
  #  group_by(name) %>%
  #  count()
  #ln
  #max_ln <- max(ln$n)
  
  # create dataframe to contain the most frequent words per org
  top_n_per_org <- data.frame(top_n %>% 
                                filter(name == word_frequency_df$name[1]) %>%
                                select(word))
  # add a column with the top words for each org
  for (i in seq(nrow(users[1]))) {
    top_n_of_org <- top_n %>% filter(name == users$name[i]) %>% select(word) # get top words of org
    top_n_per_org[[users$name[i]]] = head(top_n_of_org, n_top_words) # add top words to df
  }
  
  top_n_per_org <- top_n_per_org %>% unnest() %>% head(n_top_words) %>%
    select(!word)
  return(top_n_per_org)
}

# get the most frequently used words by each org in SA and globally
top_words_SA <- top_words(word_frequency_SA, n_words)
top_words_global <- top_words(word_frequency_global, n_words)

top_words_SA
top_words_global
```






# Top n words overall

```{r frequency of words overall (not per org) in ascending order}
n_words_overall <- 20

overall_word_frequencies <- function(tidy_tweets_df) {
  overall_frequency <- tidy_tweets_df %>%
  select(word) %>%
  count(word) %>% # count the number of times each word occurs
  arrange(n) %>% # order words to increasing nr of occureences 
  mutate(total = sum(n)) # add column with total nr of words used

  overall_frequency$word <- factor(overall_frequency$word, levels = overall_frequency$word)
  overall_frequency # create factor to keep the order
}

# calculate how often each word is used by all SA and global orgs
overall_word_frequency_SA <- overall_word_frequencies(tidy_tweets_SA)
overall_word_frequency_global <- overall_word_frequencies(tidy_tweets_global)

# extract the n_words_overall most frequently used words
top_words_SA_overall <- tail(overall_word_frequency_SA, n_words_overall)
top_words_global_overall <- tail(overall_word_frequency_global, n_words_overall)

top_words_SA_overall
top_words_global_overall
```


# Top hashtags

```{r extract hashtags}
extract_hashtags <- function(tidy_tweets_df) {
  return(tidy_tweets_df %>% filter(str_detect(word, "^#"))) # filter to only include hashtags
}
# get hashtags used by SA and global orgs
tidy_tweets_SA_hashtag <- extract_hashtags(tidy_tweets_SA)
tidy_tweets_global_hashtag <- extract_hashtags(tidy_tweets_global)
```

```{r top hashtags}
n_top_hashtags <- 5

# frequency of each hashtag per media outlet
hashtag_frequency_SA <- word_frequencies(tidy_tweets_SA_hashtag)
hashtag_frequency_global <- word_frequencies(tidy_tweets_global_hashtag)

# top hashtags per media outlet
# not enough hashtags used by individual outlets
# top_hashtags_SA <- top_words(hashtag_frequency_SA, n_top_hashtags)
# top_hashtags_global <- top_words(hashtag_frequency_global, n_top_hashtags)

# top overall hashtags used by media outlets
n_top_hashtags <- 20

# calculate how often each hashtag is used by all SA and global orgs
overall_hashtag_frequency_SA <- overall_word_frequencies(tidy_tweets_SA_hashtag)
overall_hashtag_frequency_global <- overall_word_frequencies(tidy_tweets_global_hashtag)

# extract the n_top_hashtags most frequently used hashtags
top_hashtags_SA_overall <- tail(overall_hashtag_frequency_SA, n_top_hashtags)
top_hashtags_global_overall <- tail(overall_hashtag_frequency_global, n_top_hashtags)

top_hashtags_SA_overall
top_hashtags_global_overall
```


<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Data Visualisation -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->

# Comparing Tweet Frequency over Time
```{r}
# plot tweet frequency over time by SA media outlet
ggplot(tweets_SA, aes(x = created_at, fill = name)) +
  # group tweets into 36 bins over 6 months -> 6 bins per month -> roughly 5 days
  geom_histogram(position = "identity", bins = 36, show.legend = FALSE) +
  facet_wrap(~name, ncol = 1) +
  labs(title = "Frequency of Covid-related Tweets for various SA Media Outlets",
       x = "2021", y = "Tweet Count") +
  scale_fill_manual(values = ms_colors) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  custom_column_basic_theme()

# plot tweet frequency over time by global media outlet
ggplot(tweets_global, aes(x = created_at, fill = name)) +
  # group tweets into 36 bins over 6 months -> 6 bins per month -> roughly 5 days
  geom_histogram(position = "identity", bins = 36, show.legend = FALSE) +
  facet_wrap(~name, ncol = 1) +
  labs(title = "Frequency of Covid-related Tweets for various Global Media Outlets",
       x = "2021", y = "Tweet Count") +
  scale_fill_manual(values = rev(ms_colors)) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  custom_column_basic_theme()
```

```{r}
# plot govZA tweet freq over time
ggplot() +
  # group tweets into 36 bins over 6 months -> 6 bins per month -> roughly 5 days
  geom_histogram(govZA_tweets %>% mutate(created_at = as.Date(created_at)), # all tweets
                 mapping = aes(x = created_at, fill = "All Tweets"), 
                 position = "identity", bins = 36, show.legend = TRUE) +
  geom_histogram(relevant_govZA, # covid-related tweets
                 mapping = aes(x = created_at, fill = "Tweets related to Covid"), 
                 position = "identity", bins = 36, show.legend = TRUE) +
  labs(title = "GovZA Tweet Frequency over Time", fill = "Tweets") +
    scale_color_manual(values = colors) +
  scale_fill_manual(values = ms_colors) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  custom_column_basic_theme() +
  theme(legend.position = "right")
```

```{r}
# plot tweet frequency over time by SA media outlets
ggplot() +
  # group tweets into 36 bins over 6 months -> 6 bins per month -> roughly 5 days
  geom_histogram(all_tweets_SA %>% mutate(created_at = as.Date(created_at)) %>% # all tweets
                   filter(created_at > as.Date("2021-01-01 00:00:00")), # only tweets from the beginning of Jan
                 mapping = aes(x = created_at, fill = "All Tweets"), 
                 position = "identity", bins = 36, show.legend = TRUE) +
  geom_histogram(tweets_SA, # covid-related tweets
                 mapping = aes(x = created_at, fill = "Tweets related to Covid"), 
                 position = "identity", bins = 36, show.legend = TRUE) +
  scale_y_continuous(limits = c(0, 400)) +
  facet_wrap(~name, ncol = 1) + # separate by org
  labs(title = "Frequency of Tweets for various SA Media Outlets",
       x = "2021", y = "Tweet Count", fill = "Tweets") +
    scale_color_manual(values = colors) +
  scale_fill_manual(values = ms_colors) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  custom_column_basic_theme() +
  theme(legend.position = "right")

# plot tweet frequency over time by global media outlets
ggplot() +
  # group tweets into 36 bins over 6 months -> 6 bins per month -> roughly 5 days
  geom_histogram(all_tweets_global %>% mutate(created_at = as.Date(created_at)) %>% # all tweets
                   filter(created_at > as.Date("2021-01-01 00:00:00")), # only tweets from the beginning of Jan
                 mapping = aes(x = created_at, fill = "All Tweets"), 
                 position = "identity", bins = 36, show.legend = TRUE) +
  geom_histogram(tweets_global, # covid-related tweets
                 mapping = aes(x = created_at, fill = "Tweets related to Covid"), 
                 position = "identity", bins = 36, show.legend = TRUE) +
  scale_y_continuous(limits = c(0, 400)) +
  facet_wrap(~name, ncol = 1) + # separate by org
  labs(title = "Frequency of Tweets for various Global Media Outlets",
       x = "2021", y = "Tweet Count", fill = "Tweets") +
    scale_color_manual(values = colors) +
  scale_fill_manual(values = c(ms_orange, ms_green)) +
  scale_x_date(date_breaks = "1 month", date_labels = "%b") +
  custom_column_basic_theme() +
  theme(legend.position = "right")
```



# Top words
```{r top words per outlet}
# https://stackoverflow.com/questions/31796219/grid-table-and-tablegrob-in-gridextra-package
# https://cran.r-project.org/web/packages/gridExtra/vignettes/tableGrob.html#faster-tables-an-alternative-grid-function

# plot top words per outlet as table
grid.newpage()
grid.table(top_words_SA)
grid.newpage()
grid.table(top_words_global)
```

```{r top words overall used in covid-related tweets}
# plot most frequently used words by SA Media Outlets in covid-related tweets
ggplot(top_words_SA_overall, aes(word, n)) +
  geom_col() +
  labs(title = "Top 20 words overall used by SA Media Outlets") +
  coord_flip() +
  scale_fill_manual(values = ms_colors) +
  custom_column_basic_theme()

# plot most frequently used words by global Media Outlets in covid-related tweets
ggplot(top_words_global_overall, aes(word, n)) +
  geom_col() +
  labs(title = "Top 20 words overall used by Global Media Outlets") +
  coord_flip() +
  scale_fill_manual(values = ms_colors) +
  custom_column_basic_theme()
```
# Top Hashtags

```{r}
# plot most frequently used hashtags by SA Media Outlets in covid-related tweets
ggplot(top_hashtags_SA_overall, aes(word, n)) +
  geom_col() +
  labs(title = "Top hashtags overall used by SA Media Outlets") +
  coord_flip()

# plot most frequently used hashtags by global Media Outlets in covid-related tweets
ggplot(top_hashtags_global_overall, aes(word, n)) +
  geom_col() +
  labs(title = "Top hashtags overall used by SA Media Outlets") +
  coord_flip()
```


<!----------------------------------------------------------------------------------------------------------------------------------------------------->
<!-- Decided not to use -->
<!----------------------------------------------------------------------------------------------------------------------------------------------------->

# word usage over time

```{r}
words_by_time <- tidy_tweets_SA %>%
  #filter(!str_detect(word, "^@")) %>%
  # create time bins
  mutate(time_floor = floor_date(created_at, unit = "week")) %>%
  # count how often each org uses each word in each bin
  count(time_floor, name, word) %>%
  # add column for the total number words used by each org in each time bin
  group_by(name, time_floor) %>%
  mutate(time_total = sum(n)) %>%
  # add column for the total number of times each word was used by each org
  group_by(name, word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  # consider only top words
  #filter(word_total > 35)
  filter(word %in% top_words_SA_overall$word)

words_by_time
```
```{r}
library(purrr)
library(broom)
# create a dataframe with one row for each user-word combination, remaining data as a tibble in the last column
nested_data <- words_by_time %>%
  nest(-word, -name) 
nested_data

# model to determine the change in frequency over time as slopes
nested_models <- nested_data %>%
  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., 
                                  family = "binomial")))
nested_models

# extract slopes
slopes <- nested_models %>%
  mutate(models = map(models, tidy)) %>%
  unnest(cols = c(models)) %>%
  filter(term == "time_floor") %>%
  mutate(adjusted.p.value = p.adjust(p.value))
slopes
```

```{r}
# plot change of word usage over time for SABC
words_by_time %>%
  inner_join(slopes, by = c("word", "name")) %>%
  filter(name == "SABC News") %>%
  ggplot(aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1.3) +
  labs(x = NULL, y = "Word frequency") +
  # scale_color_manual(values = ms_colors) +
  custom_line_basic_theme()
```


```{r}
# plot comparison of word usage over time by orgs

words_by_time_slopes <- words_by_time %>%
  inner_join(slopes, by = c("word", "name"))


ggplot(words_by_time_slopes, aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1.3) +
  facet_wrap(~name, ncol = 1) +
  labs(x = NULL, y = "Word frequency") +
  custom_column_basic_theme()
```

# hashtag usage over time
```{r}
ht_by_time <- tidy_tweets_SA_hashtag %>%
  # only consider hashtags
  filter(str_detect(word, "^#")) %>%
  # create time bins
  mutate(time_floor = floor_date(created_at, unit = "week")) %>%
  # count how often each org uses each hashtag in each bin
  count(time_floor, name, word) %>%
  # add column for the total number hashtags used by each org in each time bin
  group_by(name, time_floor) %>%
  mutate(time_total = sum(n)) %>%
  # add column for the total number of times each hashtag was used by each org
  group_by(name, word) %>%
  mutate(word_total = sum(n)) %>%
  ungroup() %>%
  rename(count = n) %>%
  # consider only top hashtags
   filter(word_total > 15)

ht_by_time
```
```{r}
library(purrr)
library(broom)
# create a dataframe with one row for each user-word combination, remaining data as a tibble in the last column
nested_data <- ht_by_time %>%
  nest(-word, -name) 
nested_data

# model to determine the change in frequency over time as slopes
nested_models <- nested_data %>%
  mutate(models = map(data, ~ glm(cbind(count, time_total) ~ time_floor, ., 
                                  family = "binomial")))
nested_models

# extract slopes
slopes <- nested_models %>%
  mutate(models = map(models, tidy)) %>%
  unnest(cols = c(models)) %>%
  filter(term == "time_floor") %>%
  mutate(adjusted.p.value = p.adjust(p.value))
slopes
```

```{r}
# plot change of hashtag usage over time for SABC
ht_by_time %>%
  inner_join(slopes, by = c("word", "name")) %>%
  filter(name == "SABC News") %>%
  ggplot(aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1.3) +
  labs(x = NULL, y = "Word frequency") +
  custom_line_basic_theme()
```

```{r}
# plot change of word usage over time for daily maverick
ht_by_time %>%
  inner_join(slopes, by = c("word", "name")) %>%
  filter(name == "Daily Maverick") %>%
  ggplot(aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1.3) +
  labs(x = NULL, y = "Word frequency") +
  scale_color_manual(values = ms_colors) +
  custom_line_basic_theme()
# NOTE: daily maverick has no frequent hashtags
```

```{r}
# plot comparison of word usage over time by orgs

ht_by_time_slopes <- ht_by_time %>%
  inner_join(slopes, by = c("word", "name"))


ggplot(ht_by_time_slopes, aes(time_floor, count/time_total, color = word)) +
  geom_line(size = 1.3) +
  facet_wrap(~name, ncol = 1) +
  labs(x = NULL, y = "Word frequency") +
  custom_line_basic_theme()
```
